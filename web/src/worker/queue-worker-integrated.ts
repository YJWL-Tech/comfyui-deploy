/**
 * é›†æˆåˆ° Next.js çš„ Worker å®ç°
 * 
 * æ³¨æ„ï¼šæ­¤å®ç°ä¸»è¦ç”¨äºå¼€å‘ç¯å¢ƒæˆ–ç‰¹å®šéƒ¨ç½²åœºæ™¯
 * ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ç‹¬ç«‹çš„ worker è¿›ç¨‹
 */

import { Worker } from "bullmq";
import Redis from "ioredis";
import { processQueueJob } from "./queue-worker-core";

// ä½¿ç”¨ global å¯¹è±¡å­˜å‚¨å®ä¾‹ï¼Œé˜²æ­¢ Next.js çƒ­é‡è½½æ—¶ä¸¢å¤±å®ä¾‹å¼•ç”¨
declare global {
    var _workerInstance: Worker | null | undefined;
    var _redisInstance: Redis | null | undefined;
}

// è·å–æˆ–åˆå§‹åŒ–å®ä¾‹çš„è¾…åŠ©å‡½æ•°
function getWorkerInstance(): Worker | null {
    return global._workerInstance || null;
}

function setWorkerInstance(worker: Worker | null) {
    global._workerInstance = worker;
}

function getRedisInstance(): Redis | null {
    return global._redisInstance || null;
}

function setRedisInstance(redis: Redis | null) {
    global._redisInstance = redis;
}

export function startWorker() {
    // æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº‹ä»¶é©±åŠ¨è°ƒåº¦å™¨
    if (process.env.USE_EVENT_DRIVEN_SCHEDULER === "true") {
        console.log("=".repeat(60));
        console.log("ğŸ“ Event-driven scheduler is enabled");
        console.log("   Worker will NOT auto-process jobs");
        console.log("   Jobs are processed when:");
        console.log("   1. New job is added to queue");
        console.log("   2. A job completes (/api/update-run callback)");
        console.log("=".repeat(60));
        
        // å¯åŠ¨æ—¶å¤„ç†æ‰€æœ‰ç­‰å¾…çš„ä»»åŠ¡
        (async () => {
            try {
                const { processAllAvailableJobs } = await import("@/server/queue/event-driven-scheduler");
                console.log("ğŸ“ Processing waiting jobs on startup...");
                const result = await processAllAvailableJobs();
                console.log(`âœ… Processed ${result.processedCount} waiting jobs on startup`);
            } catch (err) {
                console.error("âŒ Error processing waiting jobs on startup:", err);
            }
        })();
        
        return; // ä¸å¯åŠ¨ Worker
    }

    // å•ä¾‹ä¿æŠ¤ï¼šå¦‚æœå·²ç»å¯åŠ¨ï¼Œç›´æ¥è¿”å›
    const existingWorker = getWorkerInstance();
    if (existingWorker) {
        console.log("âš ï¸  Worker already started, skipping...");
        console.log("   (Worker instance is stored in global to survive Next.js hot reloads)");
        return;
    }

    // æ£€æŸ¥ç¯å¢ƒï¼šServerless ç¯å¢ƒä¸æ”¯æŒ
    if (process.env.VERCEL || process.env.NETLIFY) {
        console.log("âš ï¸  Skipping worker in serverless environment");
        return;
    }

    console.log("=".repeat(60));
    console.log("ğŸš€ Starting Integrated Queue Worker...");
    console.log("=".repeat(60));
    console.log(`ğŸ“… Start Time: ${new Date().toISOString()}`);
    console.log(`ğŸ”§ Redis URL: ${process.env.REDIS_URL || "redis://localhost:6379"}`);
    console.log(`âš™ï¸  Worker Concurrency: ${process.env.WORKER_CONCURRENCY || "5"}`);

    try {
        const redis = new Redis(process.env.REDIS_URL || "redis://localhost:6379", {
            maxRetriesPerRequest: null,
        });
        setRedisInstance(redis);

        // Redis è¿æ¥äº‹ä»¶
        redis.on("connect", () => {
            console.log("âœ… Redis connected successfully");
        });

        redis.on("error", (err) => {
            console.error("âŒ Redis connection error:", err);
        });

        redis.on("ready", () => {
            console.log("âœ… Redis ready");
        });

        const loadBalancerStrategy =
            (process.env.LOAD_BALANCER_STRATEGY as "round-robin" | "least-load") ||
            "least-load";

        const worker = new Worker(
            "workflow-run-queue",
            async (job) => {
                console.log(`\nğŸ“¦ [JOB ${job.id}] Processing job for deployment ${job.data.deployment_id}`);
                try {
                    // ä½¿ç”¨å…±äº«çš„æ ¸å¿ƒå¤„ç†é€»è¾‘ï¼Œä½†ç¦ç”¨è¯¦ç»†æ—¥å¿—ï¼ˆé›†æˆæ¨¡å¼é€šå¸¸ä¸éœ€è¦å¤ªå¤šæ—¥å¿—ï¼‰
                    return await processQueueJob({
                        job,
                        loadBalancerStrategy,
                        enableDetailedLogging: false, // é›†æˆæ¨¡å¼ä½¿ç”¨ç®€å•æ—¥å¿—
                    });
                } catch (error: any) {
                    // å¦‚æœæ˜¯å› ä¸º machine ä¸å¯ç”¨å¯¼è‡´çš„é”™è¯¯ï¼Œè®¾ç½®å»¶è¿Ÿé‡è¯•
                    // è¿™æ · worker å¯ä»¥ç»§ç»­å¤„ç†å…¶ä»– machine çš„ä»»åŠ¡
                    if (error?.needsDelayedRetry) {
                        // å¢åŠ é‡è¯•è®¡æ•°
                        const retryCount = (job.data.retryCount || 0) + 1;
                        const maxRetries = parseInt(process.env.MAX_QUEUE_RETRIES || "200"); // é»˜è®¤æœ€å¤šé‡è¯•200æ¬¡

                        if (retryCount > maxRetries) {
                            // è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ ‡è®°ä¸ºå¤±è´¥
                            console.error(`âŒ [JOB ${job.id}] Machine "${error.machineName}" not available after ${maxRetries} retries`);
                            console.error(`   Marking job as failed to prevent infinite retries`);
                            throw new Error(`Machine "${error.machineName}" not available after ${maxRetries} retries`);
                        }

                        // ä½¿ç”¨å›ºå®šçš„çŸ­å»¶è¿Ÿï¼ˆ30 ç§’ï¼‰
                        // åŸå› ï¼š
                        // 1. å¤§éƒ¨åˆ† job åœ¨ waiting é˜Ÿåˆ—ä¸­ç­‰å¾…ï¼Œä¸ä¼šè¢«é‡å¤æ£€æŸ¥
                        // 2. åªæœ‰ Worker å¹¶å‘æ•°çš„ job ä¼šåŒæ—¶è¢«æ£€æŸ¥
                        // 3. çŸ­å»¶è¿Ÿè®© job èƒ½å¿«é€Ÿå“åº” machine ç©ºé—²
                        // 4. 200 æ¬¡é‡è¯• Ã— 30 ç§’ = 100 åˆ†é’Ÿçš„æœ€å¤§é‡è¯•æ—¶é—´
                        //    ï¼ˆå®é™…æ’é˜Ÿæ—¶é—´å–å†³äº waiting é˜Ÿåˆ—ï¼Œä¸æ˜¯é‡è¯•æ—¶é—´ï¼‰
                        const delayMs = parseInt(process.env.QUEUE_RETRY_DELAY || "30000"); // é»˜è®¤ 30 ç§’

                        console.log(`â° [JOB ${job.id}] Machine "${error.machineName}" not available, setting delayed retry #${retryCount}/${maxRetries} (${delayMs / 1000}s)`);
                        console.log(`   Worker will continue processing other jobs from the queue`);

                        // æ›´æ–° job data ä»¥è®°å½•é‡è¯•æ¬¡æ•°
                        await job.updateData({
                            ...job.data,
                            retryCount: retryCount,
                        });

                        // ä½¿ç”¨å»¶è¿Ÿæ—¶é—´ï¼ˆéšé‡è¯•æ¬¡æ•°å¢åŠ ï¼‰
                        // ç”¨ try-catch åŒ…è£…ï¼Œå¤„ç†é”ä¸¢å¤±çš„æƒ…å†µ
                        try {
                            if (job.token) {
                                await job.moveToDelayed(Date.now() + delayMs, job.token);
                            } else {
                                console.warn(`âš ï¸  [JOB ${job.id}] No job token available, will use BullMQ default retry`);
                            }
                        } catch (moveError: any) {
                            // é”ä¸¢å¤±æˆ–å…¶ä»–é”™è¯¯ï¼Œè®© BullMQ çš„å†…ç½®é‡è¯•æœºåˆ¶å¤„ç†
                            console.warn(`âš ï¸  [JOB ${job.id}] moveToDelayed failed: ${moveError.message}`);
                            console.warn(`   Will fall back to BullMQ default retry mechanism`);
                        }

                        // æ³¨æ„ï¼šjob.moveToDelayed ä¸èƒ½ç›´æ¥ä¿®æ”¹ä¼˜å…ˆçº§
                        // ä½†æˆ‘ä»¬å·²ç»åœ¨ job.data ä¸­è®°å½•äº† retryCount
                        throw error;
                    }
                    throw error;
                }
            },
            {
                connection: redis,
                concurrency: parseInt(process.env.WORKER_CONCURRENCY || "5"),
                // é”çš„æŒç»­æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
                // æ³¨æ„ï¼šè¿™æ˜¯ Worker å¤„ç† job æœŸé—´é”çš„æœ‰æ•ˆæœŸï¼Œä¸æ˜¯æ’é˜Ÿæ—¶é—´
                // æ’é˜Ÿæ—¶ï¼ˆwaiting/delayedï¼‰ä¸éœ€è¦é”
                // è®¾ç½®ä¸º 30 åˆ†é’Ÿï¼ŒBullMQ ä¼šè‡ªåŠ¨æ¯ lockDuration/2 ç»­é”
                lockDuration: parseInt(process.env.WORKER_LOCK_DURATION || "1800000"), // é»˜è®¤ 30 åˆ†é’Ÿ
                // stalled job æ£€æŸ¥é—´éš”ï¼Œåº”è¯¥å¤§äº lockDuration
                stalledInterval: parseInt(process.env.WORKER_STALLED_INTERVAL || "1800000"), // é»˜è®¤ 30 åˆ†é’Ÿ
            },
        );

        // Worker å°±ç»ªäº‹ä»¶
        worker.on("ready", () => {
            console.log("=".repeat(60));
            console.log("âœ… Integrated Queue Worker is ready and listening for jobs");
            console.log(`   Queue Name: workflow-run-queue`);
            console.log(`   Concurrency: ${parseInt(process.env.WORKER_CONCURRENCY || "5")}`);
            console.log(`   Load Balancer: ${loadBalancerStrategy}`);
            console.log(`   Ready at: ${new Date().toISOString()}`);
            console.log("=".repeat(60));
            console.log("ğŸ“ Worker is now processing jobs (traditional mode)...\n");
        });

        worker.on("active", (job) => {
            console.log(`ğŸ”„ [JOB ${job.id}] Job is now active (being processed)`);
        });

        worker.on("completed", (job) => {
            if (job) {
                console.log(`\nâœ… [JOB ${job.id}] Completed successfully`);
                if (job.returnvalue && typeof job.returnvalue === "object" && "workflow_run_id" in job.returnvalue) {
                    console.log(`   Workflow Run ID: ${job.returnvalue.workflow_run_id}`);
                }
            }
        });

        worker.on("failed", async (job, err) => {
            console.log("\n" + "=".repeat(60));
            if (job) {
                console.error(`âŒ [JOB ${job.id}] Failed`);
                console.error(`   Error:`, err);
                console.error(`   Attempts: ${job.attemptsMade}`);
                if (job.failedReason) {
                    console.error(`   Reason: ${job.failedReason}`);
                }
                
                // å‘é€å¤±è´¥é€šçŸ¥ï¼ˆå³ä½¿æ²¡æœ‰ workflow_run è®°å½•ï¼‰
                try {
                    const webhookUrl = process.env.WEBHOOK_NOTIFICATION_URL;
                    if (webhookUrl) {
                        const { enqueueNotification } = await import("@/server/notifications/notification-queue");
                        const payload = {
                            workflow_run_id: `queue-job-${job.id}`, // ä½¿ç”¨ job_id ä½œä¸ºæ ‡è¯†
                            status: "failed" as const,
                            job_id: job.id,
                            deployment_id: job.data.deployment_id,
                            error: err.message || "Unknown error",
                            completed_at: new Date().toISOString(),
                            webhook_url: webhookUrl,
                            webhook_auth_header: process.env.WEBHOOK_AUTHORIZATION_HEADER,
                        };
                        await enqueueNotification(payload);
                        console.log(`âœ… [JOB ${job.id}] Failure notification enqueued`);
                    }
                } catch (notifyError) {
                    console.error(`âŒ [JOB ${job.id}] Failed to enqueue failure notification:`, notifyError);
                }
            } else {
                console.error("âŒ Job failed (job info unavailable)");
                console.error(`   Error:`, err);
            }
            console.log("=".repeat(60) + "\n");
        });

        worker.on("error", (err) => {
            console.error("\n" + "=".repeat(60));
            console.error("âŒ Worker error occurred");
            console.error(`   Time: ${new Date().toISOString()}`);
            console.error(`   Error:`, err);
            console.log("=".repeat(60) + "\n");
        });

        worker.on("stalled", (jobId) => {
            console.warn(`âš ï¸  [JOB ${jobId}] Job stalled (may be taking too long)`);
        });

        setWorkerInstance(worker);

        // æ£€æŸ¥ Redis è¿æ¥
        redis.ping()
            .then(() => {
                console.log("âœ… Redis ping successful");
            })
            .catch((err) => {
                console.error("âŒ Redis ping failed:", err);
                console.error("   Please check if Redis is running and accessible");
            });

        // ä¼˜é›…å…³é—­
        const cleanup = async () => {
            console.log("\nğŸ›‘ Closing integrated worker...");
            const worker = getWorkerInstance();
            if (worker) {
                await worker.close();
                setWorkerInstance(null);
            }
            const redis = getRedisInstance();
            if (redis) {
                await redis.quit();
                setRedisInstance(null);
            }
            console.log("âœ… Worker closed gracefully");
        };

        process.on("SIGTERM", cleanup);
        process.on("SIGINT", cleanup);

        console.log("â³ Waiting for worker to be ready...");
    } catch (error) {
        console.error("âŒ Failed to start integrated worker:", error);
        console.error("   Error details:", error instanceof Error ? error.stack : String(error));
    }
}

export async function stopWorker(force: boolean = false) {
    console.log("ğŸ›‘ Stopping integrated worker...");
    if (force) {
        console.log("âš ï¸  Force stop enabled - active jobs will be interrupted");
    } else {
        console.log("â„¹ï¸  Graceful stop - waiting for active jobs to complete");
    }

    const workerInstance = getWorkerInstance();
    const redisInstance = getRedisInstance();

    if (workerInstance) {
        try {
            // æ£€æŸ¥å½“å‰é˜Ÿåˆ—çŠ¶æ€ï¼ˆä»…ç”¨äºæ—¥å¿—ï¼‰
            if (redisInstance) {
                try {
                    const { Queue } = await import("bullmq");
                    const queue = new Queue("workflow-run-queue", {
                        connection: redisInstance,
                    });
                    const [waiting, active] = await Promise.all([
                        queue.getWaitingCount(),
                        queue.getActiveCount(),
                    ]);
                    console.log(`ğŸ“Š Queue status before stop: waiting=${waiting}, active=${active}`);
                    console.log(`â„¹ï¸  Note: Completed jobs do not affect worker stop, only active jobs do`);
                    await queue.close();
                } catch (err) {
                    // å¿½ç•¥æ£€æŸ¥é”™è¯¯ï¼Œç»§ç»­åœæ­¢
                    console.log("âš ï¸  Could not check queue status:", err);
                }
            }

            // BullMQ çš„ close æ–¹æ³•
            // æ³¨æ„ï¼šå·²å®Œæˆï¼ˆcompletedï¼‰çš„ä»»åŠ¡ä¸ä¼šå½±å“åœæ­¢ï¼Œåªæœ‰ active ä»»åŠ¡ä¼šå½±å“
            // ä½¿ç”¨ force=true ä¼šç«‹å³åœæ­¢ï¼Œä¸ç­‰å¾… active ä»»åŠ¡å®Œæˆ
            if (force) {
                // å¼ºåˆ¶åœæ­¢ï¼šç«‹å³åœæ­¢ï¼Œä¸ç­‰å¾…ä»»åŠ¡å®Œæˆ
                await workerInstance.close(true);
            } else {
                // ä¼˜é›…åœæ­¢ï¼šç­‰å¾… active ä»»åŠ¡å®Œæˆ
                await workerInstance.close();
            }
            console.log("âœ… Worker closed");
            setWorkerInstance(null);
        } catch (error) {
            console.error("âŒ Error closing worker:", error);
            // å³ä½¿å‡ºé”™ä¹Ÿè¦æ¸…ç©ºå®ä¾‹
            setWorkerInstance(null);
        }
    } else {
        console.log("â„¹ï¸  Worker is not running (no instance found)");
    }

    if (redisInstance) {
        try {
            await redisInstance.quit();
            console.log("âœ… Redis connection closed");
            setRedisInstance(null);
        } catch (error) {
            console.error("âŒ Error closing Redis:", error);
            setRedisInstance(null);
        }
    }

    console.log("âœ… Worker stopped successfully");
}

